{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher's Exact Test for Proportions\n",
    "> Use Case\n",
    "- Fisher's exact test is ideal for A/B hypothesis testing when dealing with small sample sizes and categorical data in a 2x2 contingency table, however it can also be used with large sample sizes. \n",
    "- It is typically used to determine whether there is a significant association between two categorical variables, such as treatment and outcome.\n",
    "- It provides exact results rather than approximations like the Chi-Squared test.\n",
    "> Assumptions\n",
    "- Independence: The observations in each group are independent of each other.\n",
    "- 2x2 Contingency Table: The test is designed to handle two categorical variables with two levels each.\n",
    "- Small Sample Size: Unlike the Chi-Squared test, Fisher's exact test can handle small sample sizes effectively, which makes it useful in scenarios where expected counts are low.\n",
    "- No Homogeneity Assumptions: The test does not require equal variance across groups.\n",
    "> Example Scenario\n",
    "- A company is testing two versions of an email marketing campaign (A and B) to determine which one yields a higher response rate. The response (open or not open) is recorded for each email recipient, creating a 2x2 contingency table. The company wants to determine if there is a significant difference in the response rates between the two campaigns.\n",
    "> Null Hypothesis (H0):\n",
    "- The null hypothesis is that there is no association between the treatment (email campaigns A and B) and the outcome (response rates). This implies that the proportions of responses are the same for both campaigns.\n",
    "> Alternative Hypothesis (H1):\n",
    "- The alternative hypothesis is that there is a significant association between the treatment and the outcome, indicating a difference in response rates between the two campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# !pip install scipy==1.12.0\n",
    "from scipy.stats import fisher_exact, barnard_exact, boschloo_exact, norm\n",
    "\n",
    "# !pip install statsmodels==0.12.2\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seaborn style\n",
    "sns.set(style=\"white\", palette=\"mako\")\n",
    "\n",
    "# colors\n",
    "color = 'tab:orange'\n",
    "\n",
    "# remove some borders    \n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "plt.rcParams['axes.spines.left'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate Fisher's exact test\n",
    "def simulate_fisher(n_samples, p1, p2, alpha):\n",
    "    \"\"\"Simulates Fisher's exact test and returns True if p-value < alpha.\"\"\"\n",
    "    group1_success = np.random.binomial(n_samples, p1)\n",
    "    group2_success = np.random.binomial(n_samples, p2)\n",
    "    \n",
    "    # 2x2 contingency table\n",
    "    contingency_table = np.array([\n",
    "        [group1_success, n_samples - group1_success],\n",
    "        [group2_success, n_samples - group2_success]\n",
    "    ])\n",
    "    \n",
    "    _, p_value = fisher_exact(contingency_table)\n",
    "    return p_value < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Simulation parameters\n",
    "alpha = 0.05  # Significance level\n",
    "desired_power = 0.8  # Desired power level\n",
    "max_samples = 1000  # Maximum sample size to consider in simulations\n",
    "n_simulations = 1000  # Number of simulations to estimate power\n",
    "effect_sizes = np.linspace(0.1, 0.5, 10)  # Range of effect sizes to test\n",
    "baseline_p = 0.2 # The historical probability (CTR, etc...) of the control group\n",
    "\n",
    "\n",
    "# Determine required sample size for each effect size\n",
    "sample_sizes = np.arange(10, max_samples + 10, 10)  # Sample sizes in steps of 10\n",
    "required_sample_sizes = []\n",
    "\n",
    "for effect_size in effect_sizes:\n",
    "    # Define probabilities for the groups based on effect size\n",
    "    p1 = baseline_p  # Baseline probability\n",
    "    p2 = p1 + effect_size  # Probability for the second group\n",
    "    \n",
    "    # Find the minimum sample size to achieve the desired power\n",
    "    power_reached = False\n",
    "    for sample_size in sample_sizes:\n",
    "        rejections = [simulate_fisher(sample_size, p1, p2, alpha) for _ in range(n_simulations)]\n",
    "        power = sum(rejections) / n_simulations\n",
    "        \n",
    "        if power >= desired_power:\n",
    "            required_sample_sizes.append(sample_size)\n",
    "            break\n",
    "\n",
    "# Plot effect sizes against required sample sizes\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(effect_sizes, required_sample_sizes, marker='o', linestyle='-', color=color, label='Required Sample Size')\n",
    "plt.fill_between(effect_sizes, required_sample_sizes, color=color, alpha=0.3)\n",
    "plt.title('Required Sample Size vs. Effect Size for 80% Power (Fisher\\'s Exact Test)')\n",
    "plt.xlabel('Effect Size')\n",
    "plt.ylabel('Required Sample Size')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulation parameters\n",
    "alpha = 0.05  # Significance level\n",
    "desired_power = 0.8  # Desired power level\n",
    "max_samples = 1000  # Maximum sample size to consider in simulations\n",
    "n_simulations = 1000  # Number of simulations to estimate power\n",
    "effect_size = 0.15  # You can adjust this depending on your expected effect size\n",
    "baseline_p = 0.2 # The historical probability (CTR, etc...) of the control group\n",
    "estimated_p = baseline_p + effect_size # The estimated probability (CTR, etc...) of the new group\n",
    "\n",
    "# Simulation-based power analysis\n",
    "sample_sizes = np.arange(10, max_samples + 10, 10)  # Increasing sample sizes in steps of 10\n",
    "powers = []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    # Simulate Fisher's exact test 1000 times to estimate power\n",
    "    rejections = [simulate_fisher(sample_size, baseline_p, estimated_p, alpha) for _ in range(1000)]\n",
    "    power = sum(rejections) / 1000  # Proportion of rejections is the power\n",
    "    powers.append(power)\n",
    "\n",
    "# Determine the minimum sample size to achieve the desired power\n",
    "min_sample_size = sample_sizes[np.where(np.array(powers) >= desired_power)[0][0]]\n",
    "\n",
    "print(\"Sample Size to Achieve Desired Power:\", min_sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples for each version\n",
    "n_samples = 200\n",
    "\n",
    "# Generate synthetic data for two website versions\n",
    "version_A = np.random.choice(['Satisfied', 'Not Satisfied'], size=n_samples, p=[0.8, 0.2])\n",
    "version_B = np.random.choice(['Satisfied', 'Not Satisfied'], size=n_samples, p=[0.75, 0.25])\n",
    "\n",
    "# Create pandas dataframe\n",
    "fischers_data = pd.DataFrame({'Version_A': version_A, 'Version_B': version_B})\n",
    "\n",
    "fischers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table (the wrong way - this is paired samples)\n",
    "observed_counts = pd.crosstab(fischers_data['Version_A'], fischers_data['Version_B'])\n",
    "observed_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table (the right way - this is independent counts)\n",
    "observed_counts = fischers_data.melt().rename(columns = {'variable':'Group', 'value':'Satisfied'})\n",
    "observed_counts = pd.crosstab(observed_counts['Group'], observed_counts['Satisfied'])\n",
    "observed_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_counts = observed_counts.sample(frac=1)\n",
    "observed_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the contingency table\n",
    "print(\"Contingency Table:\")\n",
    "display(observed_counts)\n",
    "\n",
    "# Check if the table is 2x2\n",
    "if observed_counts.shape == (2, 2):\n",
    "    print(\"The contingency table is 2x2.\")\n",
    "else:\n",
    "    print(\"The contingency table is not 2x2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fishers's Test (scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-determined alpha\n",
    "alpha = 0.05  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform Fisher's exact test\n",
    "odds_ratio, p_value = fisher_exact(observed_counts, alternative='two-sided')\n",
    "\n",
    "# Print the odds ratio and p-value\n",
    "print(\"Fisher's Exact Test Results:\")\n",
    "print(f\"Odds Ratio: {odds_ratio}, P-value: {p_value}\")\n",
    "\n",
    "\n",
    "# Check for statistical significance\n",
    "if p_value < alpha:\n",
    "    conclusion = \"Reject the null hypothesis.\"\n",
    "    interpretation = \"There is a significant difference in the proportions between Group A and Group B.\"\n",
    "else:\n",
    "    conclusion = \"Fail to reject the null hypothesis.\"\n",
    "    interpretation = \"There is no significant difference in proportions between Group A and Group B.\"\n",
    "print(\"\\n\")\n",
    "print(conclusion)\n",
    "print(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barnard's Exact Test (SciPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Barnard's exact test\n",
    "ber = barnard_exact(observed_counts, alternative='two-sided', pooled=True, n=32)\n",
    "\n",
    "wald_statistic = ber.statistic\n",
    "p_value = ber.pvalue\n",
    "\n",
    "\n",
    "# Print the odds ratio and p-value\n",
    "print(\"Barnard's Exact Test Results:\")\n",
    "print(f\"Wald Statistic: {odds_ratio}, P-value: {p_value}\")\n",
    "\n",
    "\n",
    "# Check for statistical significance\n",
    "if p_value < alpha:\n",
    "    conclusion = \"Reject the null hypothesis.\"\n",
    "    interpretation = \"There is a significant difference in the proportions between Group A and Group B.\"\n",
    "else:\n",
    "    conclusion = \"Fail to reject the null hypothesis.\"\n",
    "    interpretation = \"There is no significant difference in proportions between Group A and Group B.\"\n",
    "print(\"\\n\")\n",
    "print(conclusion)\n",
    "print(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boschloo's Exact Test (SciPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform Boschloo's exact test\n",
    "ber = boschloo_exact(observed_counts, alternative='two-sided', n=32)\n",
    "\n",
    "statistic = ber.statistic\n",
    "p_value = ber.pvalue\n",
    "\n",
    "# Print the odds ratio and p-value\n",
    "print(\"Boschloo's Exact Test Results:\")\n",
    "print(f\"Statistic: {statistic}, P-value: {p_value}\")\n",
    "\n",
    "\n",
    "# Check for statistical significance\n",
    "if p_value < alpha:\n",
    "    conclusion = \"Reject the null hypothesis.\"\n",
    "    interpretation = \"There is a significant difference in the proportions between Group A and Group B.\"\n",
    "else:\n",
    "    conclusion = \"Fail to reject the null hypothesis.\"\n",
    "    interpretation = \"There is no significant difference in proportions between Group A and Group B.\"\n",
    "print(\"\\n\")\n",
    "print(conclusion)\n",
    "print(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 95% confidence interval\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Proportions of satisfied users\n",
    "p1 = (fischers_data['Version_B'] == 'Satisfied').mean()\n",
    "p2 = (fischers_data['Version_A'] == 'Satisfied').mean()\n",
    "\n",
    "\n",
    "# Total number of samples in each group\n",
    "n1 = n_samples\n",
    "n2 = n_samples\n",
    "\n",
    "# Calculate the difference in proportions\n",
    "diff_proportions = p2 - p1\n",
    "\n",
    "# Calculate the standard error for the difference in proportions\n",
    "standard_error = np.sqrt((p1 * (1 - p1) / n1) + (p2 * (1 - p2) / n2))\n",
    "\n",
    "# Calculate the margin of error (1.96 for 95% confidence interval)\n",
    "margin_of_error = norm.ppf(1 - (1 - confidence_level) / 2) * standard_error  \n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "lower_bound = diff_proportions - margin_of_error\n",
    "upper_bound = diff_proportions + margin_of_error\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Difference in Proportions: {diff_proportions}\")\n",
    "print(f\"95% Confidence Interval of the Difference in Proportions: [{lower_bound}, {upper_bound}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohen's h effect size\n",
    "h = abs(proportion_effectsize(p1, p2, method='normal'))\n",
    "\n",
    "print(f\"Cohen's h Effect Size: {h}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "if h >= 0.8:\n",
    "    print(\"There is a large difference between the two groups.\")\n",
    "elif h >= 0.5:\n",
    "    print(\"There is a moderate difference between the two groups.\")\n",
    "elif h >= 0.2:\n",
    "    print(\"There is a small difference between the two groups.\")\n",
    "else:\n",
    "    print(\"There is no difference between the two groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Odds Ratio\n",
    "print(f\"The odds ratio is {round(odds_ratio, 4)}\")\n",
    "print(f\"The odds of the outcome are {round(odds_ratio, 4)} times as high in the exposed group compared to the unexposed group.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "if odds_ratio > 1:\n",
    "    print(\"This suggests a positive association between the exposure and the outcome.\\nIt means that the odds of the outcome occurring in the exposed group are higher than the odds of the outcome occurring in the unexposed group. \")\n",
    "elif odds_ratio < 1:\n",
    "    print(\"This implies a negative association between the exposure and the outcome.\\nIt indicates that the odds of the outcome occurring in the exposed group are lower than the odds of the outcome occurring in the unexposed group.\")\n",
    "else:\n",
    "    print(\"This indicates no association between the exposure and the outcome.\\nIn other words, the odds of the outcome occurring in the exposed group are the same as the odds of the outcome occurring in the unexposed group.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count everything up\n",
    "total_count = fischers_data.size\n",
    "group_a_count = fischers_data['Version_A'].count()\n",
    "group_b_count = fischers_data['Version_B'].count()\n",
    "\n",
    "group_a_positive_count = len(fischers_data[fischers_data['Version_A'] == 'Satisfied'])\n",
    "group_a_negative_count = len(fischers_data[fischers_data['Version_A'] == 'Not Satisfied'])\n",
    "group_b_positive_count = len(fischers_data[fischers_data['Version_B'] == 'Satisfied'])\n",
    "group_b_negative_count = len(fischers_data[fischers_data['Version_B'] == 'Not Satisfied'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'Intervention Group': [\"Group A\", \"Group B\"],\n",
    "    'Total': [group_a_count, group_b_count],\n",
    "    'Satisfied': [group_a_positive_count, group_b_positive_count],\n",
    "    'Not Satisfied': [group_a_negative_count, group_b_negative_count],\n",
    "    'Satisfaction Rate': [group_a_positive_count/group_a_count, group_b_positive_count/group_b_count],\n",
    "}, index=[0, 1])\n",
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stacked bars\n",
    "plt.figure(figsize=(6, 10))\n",
    "\n",
    "plot_total = sns.barplot(x=plot_data['Intervention Group'], \n",
    "                        y=plot_data['Total'], color='lightgrey')\n",
    "\n",
    "plot_satisfied = sns.barplot(x=plot_data['Intervention Group'], \n",
    "                            y=plot_data['Satisfied'], color=color)\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Satisfaction Rate per Intervention Group', fontsize=15)\n",
    "plt.ylabel('Count')\n",
    "plt.tick_params(left=False,bottom=False,labelleft=True,labelbottom=True)\n",
    "\n",
    "# Put values on top of bars\n",
    "row_list = []\n",
    "for index, row in plot_data[plot_data['Intervention Group'] == 'Group A'][['Satisfied', 'Satisfaction Rate']].iterrows():\n",
    "    plot_total.text(row.name, row['Satisfied']+2, str(round(100 * row['Satisfaction Rate'], 2)) + '%', color=color, ha=\"center\", fontsize=15)\n",
    "for index, row in plot_data[plot_data['Intervention Group'] == 'Group B'][['Satisfied', 'Satisfaction Rate']].iterrows():\n",
    "    plot_satisfied.text(row.name, row['Satisfied']+2, str(round(100 * row['Satisfaction Rate'], 2)) + '%', color=color, ha=\"center\", fontsize=15)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
